# アンサンブル学習とは
# 複数の識別器の結果を集合して最適な識別器を作る機械学習手法

# バギングとは
# データセットから複数のサンプル（検証用データセットと学習用データセット）を作成する
# バギングではブートストラップ法でサンプル作成
# 各サンプルで目的のモデルを作成
# 各サンプルで作成したモデルの推定値を回帰の場合平均値で、分類の場合は多数決で最終的な推定値を算出する
# データセットが小さいデータでも使用ができる

# ランダムフォレストとは
# 基本的なアルゴリズムはバギングと一緒
# バギングと異なる点
# 決定木を作る説明変数をランダムに選ぶ
# →ランダムフォレストでは全ての説明変数を使わず、一部の説明変数のみ使用する
# 決定木の深さ、つまり分岐（ノード）の数を制限する
# 決定木の剪定を最初から取り入れている
# これらの何が良いか
# たくさんのサンプルから作成される決定木にバリエーションが生まれる
# 徹底的な学習をさせないため、似たような決定木が生まれづらい
# これにより、過学習を防いでいる→汎化性能を上げている

# スタッキング
# 複数段階に分けて学習・予測をしていく
# 今回２段階のものを考える
# 1段階目には普通に学習データを作ってモデルを複数構築する
# 2段階目では、それぞれのモデルの予測値を学習データとしてさらにモデルを作成する
# 何が良いか
# 1段階目で生じたモデルの高い共変量や高いバイアスを最終的に調整することで、高い精度を出すことができる。


# ブースティングとは
# 弱学習器での結果を踏まえ、全体の重み調整を行い、学習精度を上げる手法、学習器をレベルアップしていく手法
# ↓アダブースティングフロー
# データの一部を決められた回数分抽出して弱学習器を作成する（同じデータに対して学習）
# まず一つ目の学習器でモデル作成・予測を行う
# 学習器の誤った一つの結果と、正解のサンプルを比べる
# データセットの中で間違いやすいデータと間違いにくいデータが出てくる
# 誤り率と重要度を計算する
# 間違えやすいデータは重み(ウエイト)が大きくなり、正しければ重みは小さくしていきます。
# こうすることで、次に学習する弱学習器が誤っているデータを重点的に学習してくれるようになるということです。
# 計算毎に全体の重みを調節する

# データそれぞれに重みをつけるのではなく、誤差を修正していくモデルも考えられる
# 勾配ブースティング
# 正解との誤差の勾配を用いて算出、
# 誤差を修正するためにどれくらい数値を修正するかを次の学習器で学習させる
# 上記で変化を加えた結果をさらに同様のフローで予測を修正していく
# 学習器を増やすたびに正解との誤差を修正していき、最終的に精度の高いモデルを作成していく
